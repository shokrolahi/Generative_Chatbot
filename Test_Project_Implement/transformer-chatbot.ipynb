{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:10.220861Z","iopub.status.busy":"2023-05-30T09:43:10.220086Z","iopub.status.idle":"2023-05-30T09:43:29.179160Z","shell.execute_reply":"2023-05-30T09:43:29.178158Z","shell.execute_reply.started":"2023-05-30T09:43:10.220828Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Aspire_Data\\ClassAssignment\\Semester-3\\Capstone\\Test_Project_Implement\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","from time import time\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","#!pip install tensorflow-datasets==1.2.0\n","import tensorflow_datasets as tfds"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-datasets==1.2.0 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (1.2.0)\n","Requirement already satisfied: absl-py in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.4.0)\n","Requirement already satisfied: attrs in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (23.1.0)\n","Requirement already satisfied: dill in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.3.7)\n","Requirement already satisfied: future in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.18.3)\n","Requirement already satisfied: numpy in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.23.5)\n","Requirement already satisfied: promise in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.3)\n","Requirement already satisfied: protobuf>=3.6.1 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (3.20.3)\n","Requirement already satisfied: psutil in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (5.9.5)\n","Requirement already satisfied: requests>=2.19.0 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.31.0)\n","Requirement already satisfied: six in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.16.0)\n","Requirement already satisfied: tensorflow-metadata in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.14.0)\n","Requirement already satisfied: termcolor in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.3.0)\n","Requirement already satisfied: tqdm in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (4.66.1)\n","Requirement already satisfied: wrapt in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2023.7.22)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets==1.2.0) (1.60.0)\n","Requirement already satisfied: colorama in c:\\aspire_data\\classassignment\\semester-3\\capstone\\test_project_implement\\.venv\\lib\\site-packages (from tqdm->tensorflow-datasets==1.2.0) (0.4.6)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.2.1 -> 23.3\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:29.183330Z","iopub.status.busy":"2023-05-30T09:43:29.182728Z","iopub.status.idle":"2023-05-30T09:43:29.209455Z","shell.execute_reply":"2023-05-30T09:43:29.208442Z","shell.execute_reply.started":"2023-05-30T09:43:29.183302Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["REPLICAS: 1\n"]}],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()\n","\n","print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:58:44.811236Z","iopub.status.busy":"2023-05-30T09:58:44.810303Z","iopub.status.idle":"2023-05-30T09:58:44.817167Z","shell.execute_reply":"2023-05-30T09:58:44.816167Z","shell.execute_reply.started":"2023-05-30T09:58:44.811188Z"},"trusted":true},"outputs":[],"source":["# Maximum sentence length\n","MAX_LENGTH = 100\n","\n","# For tf.data.Dataset\n","BATCH_SIZE = int(64 * strategy.num_replicas_in_sync)\n","# BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","# For Transformer\n","NUM_LAYERS = 2 #6\n","D_MODEL = 256 #512\n","NUM_HEADS = 8\n","UNITS = 512 #2048\n","DROPOUT = 0.1\n","\n","EPOCHS = 500"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:29.219715Z","iopub.status.busy":"2023-05-30T09:43:29.219317Z","iopub.status.idle":"2023-05-30T09:43:29.374177Z","shell.execute_reply":"2023-05-30T09:43:29.373292Z","shell.execute_reply.started":"2023-05-30T09:43:29.219640Z"},"trusted":true},"outputs":[],"source":["def textPreprocess(input_text):\n","\n","  def removeAccents(input_text):\n","      strange='ąćęłńóśżź'\n","      ascii_replacements='acelnoszz'\n","      translator=str.maketrans(strange,ascii_replacements)\n","      return input_text.translate(translator)\n","\n","  def removeSpecial(input_text):\n","      special='[^A-Za-z0-9 ]+'\n","      return re.sub(special, '', input_text)\n","\n","  def removeTriplicated(input_text):\n","      return re.compile(r'(.)\\1{2,}', re.IGNORECASE).sub(r'\\1', input_text)\n","\n","  return removeTriplicated(removeSpecial(removeAccents(input_text.lower())))\n","\n","\n","df = pd.read_csv(\"Conversation.csv\")\n","df['question'] = df['question'].apply(lambda x: textPreprocess(str(x)))\n","df['answer'] = df['answer'].apply(lambda x: textPreprocess(str(x)))\n","questions, answers = df['question'].tolist(), df['answer'].tolist()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:29.376061Z","iopub.status.busy":"2023-05-30T09:43:29.375601Z","iopub.status.idle":"2023-05-30T09:43:33.109726Z","shell.execute_reply":"2023-05-30T09:43:33.108627Z","shell.execute_reply.started":"2023-05-30T09:43:29.376028Z"},"trusted":true},"outputs":[],"source":["# Build tokenizer using tfds for both questions and answers\n","tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)\n","\n","# Define start and end token to indicate the start and end of a sentence\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","\n","# Vocabulary size plus start and end token\n","VOCAB_SIZE = tokenizer.vocab_size + 2"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:33.111711Z","iopub.status.busy":"2023-05-30T09:43:33.111222Z","iopub.status.idle":"2023-05-30T09:43:33.397825Z","shell.execute_reply":"2023-05-30T09:43:33.396594Z","shell.execute_reply.started":"2023-05-30T09:43:33.111653Z"},"trusted":true},"outputs":[],"source":["# Tokenize, filter and pad sentences\n","def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","  \n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # tokenize sentence\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","    # check tokenized sentence max length\n","    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n","      tokenized_inputs.append(sentence1)\n","      tokenized_outputs.append(sentence2)\n","  \n","  # pad tokenized sentences\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","  \n","  return tokenized_inputs, tokenized_outputs\n","\n","\n","questions, answers = tokenize_and_filter(questions, answers)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:33.400127Z","iopub.status.busy":"2023-05-30T09:43:33.399723Z","iopub.status.idle":"2023-05-30T09:43:33.406389Z","shell.execute_reply":"2023-05-30T09:43:33.405082Z","shell.execute_reply.started":"2023-05-30T09:43:33.400088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size: 3639\n","Number of samples: 11175\n"]}],"source":["print('Vocab size: {}'.format(VOCAB_SIZE))\n","print('Number of samples: {}'.format(len(questions)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:33.412411Z","iopub.status.busy":"2023-05-30T09:43:33.412025Z","iopub.status.idle":"2023-05-30T09:43:39.093166Z","shell.execute_reply":"2023-05-30T09:43:39.090213Z","shell.execute_reply.started":"2023-05-30T09:43:33.412376Z"},"trusted":true},"outputs":[],"source":["# decoder inputs use the previous target as input\n","# remove START_TOKEN from targets\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': questions,\n","        'dec_inputs': answers[:, :-1]\n","    },\n","    {\n","        'outputs': answers[:, 1:]\n","    },\n","))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.094833Z","iopub.status.busy":"2023-05-30T09:43:39.094488Z","iopub.status.idle":"2023-05-30T09:43:39.106242Z","shell.execute_reply":"2023-05-30T09:43:39.105016Z","shell.execute_reply.started":"2023-05-30T09:43:39.094800Z"},"trusted":true},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value, mask):\n","  \"\"\"Calculate the attention weights. \"\"\"\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # scale matmul_qk\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # add the mask to zero out padding tokens\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # softmax is normalized on the last axis (seq_len_k)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.108324Z","iopub.status.busy":"2023-05-30T09:43:39.107707Z","iopub.status.idle":"2023-05-30T09:43:39.128561Z","shell.execute_reply":"2023-05-30T09:43:39.127662Z","shell.execute_reply.started":"2023-05-30T09:43:39.108289Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # linear layers\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # split heads\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # scaled dot-product attention\n","    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # concatenation of heads\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # final linear layer\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.131382Z","iopub.status.busy":"2023-05-30T09:43:39.130404Z","iopub.status.idle":"2023-05-30T09:43:39.157135Z","shell.execute_reply":"2023-05-30T09:43:39.156038Z","shell.execute_reply.started":"2023-05-30T09:43:39.131313Z"},"trusted":true},"outputs":[],"source":["def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, sequence length)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x)\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","    # apply sin to even index in the array\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","    # apply cos to odd index in the array\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = tf.concat([sines, cosines], axis=-1)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.165809Z","iopub.status.busy":"2023-05-30T09:43:39.162646Z","iopub.status.idle":"2023-05-30T09:43:39.184224Z","shell.execute_reply":"2023-05-30T09:43:39.182737Z","shell.execute_reply.started":"2023-05-30T09:43:39.165766Z"},"trusted":true},"outputs":[],"source":["def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': padding_mask\n","      })\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.193729Z","iopub.status.busy":"2023-05-30T09:43:39.190305Z","iopub.status.idle":"2023-05-30T09:43:39.205865Z","shell.execute_reply":"2023-05-30T09:43:39.204762Z","shell.execute_reply.started":"2023-05-30T09:43:39.193694Z"},"trusted":true},"outputs":[],"source":["def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","  for i in range(int(num_layers)):\n","    outputs = encoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.211130Z","iopub.status.busy":"2023-05-30T09:43:39.210748Z","iopub.status.idle":"2023-05-30T09:43:39.228149Z","shell.execute_reply":"2023-05-30T09:43:39.227325Z","shell.execute_reply.started":"2023-05-30T09:43:39.211101Z"},"trusted":true},"outputs":[],"source":["def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': look_ahead_mask\n","      })\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1,\n","          'key': enc_outputs,\n","          'value': enc_outputs,\n","          'mask': padding_mask\n","      })\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.234795Z","iopub.status.busy":"2023-05-30T09:43:39.232386Z","iopub.status.idle":"2023-05-30T09:43:39.248637Z","shell.execute_reply":"2023-05-30T09:43:39.247857Z","shell.execute_reply.started":"2023-05-30T09:43:39.234756Z"},"trusted":true},"outputs":[],"source":["def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","  \n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  for i in range(int(num_layers)):\n","    outputs = decoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.254969Z","iopub.status.busy":"2023-05-30T09:43:39.249889Z","iopub.status.idle":"2023-05-30T09:43:39.276267Z","shell.execute_reply":"2023-05-30T09:43:39.275057Z","shell.execute_reply.started":"2023-05-30T09:43:39.254926Z"},"trusted":true},"outputs":[],"source":["def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","  # mask the future tokens for decoder inputs at the 1st attention block\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask,\n","      output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","  # mask the encoder outputs for the 2nd attention block\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  enc_outputs = encoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask])\n","\n","  dec_outputs = decoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.280547Z","iopub.status.busy":"2023-05-30T09:43:39.280016Z","iopub.status.idle":"2023-05-30T09:43:39.295464Z","shell.execute_reply":"2023-05-30T09:43:39.294486Z","shell.execute_reply.started":"2023-05-30T09:43:39.280506Z"},"trusted":true},"outputs":[],"source":["def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  \n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.299376Z","iopub.status.busy":"2023-05-30T09:43:39.299098Z","iopub.status.idle":"2023-05-30T09:43:39.307955Z","shell.execute_reply":"2023-05-30T09:43:39.307073Z","shell.execute_reply.started":"2023-05-30T09:43:39.299352Z"},"trusted":true},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.311135Z","iopub.status.busy":"2023-05-30T09:43:39.310840Z","iopub.status.idle":"2023-05-30T09:43:39.348931Z","shell.execute_reply":"2023-05-30T09:43:39.348077Z","shell.execute_reply.started":"2023-05-30T09:43:39.311112Z"},"trusted":true},"outputs":[],"source":["# clear backend\n","tf.keras.backend.clear_session()\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","def accuracy(y_true, y_pred):\n","  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:39.350511Z","iopub.status.busy":"2023-05-30T09:43:39.350095Z","iopub.status.idle":"2023-05-30T09:43:42.893018Z","shell.execute_reply":"2023-05-30T09:43:42.892209Z","shell.execute_reply.started":"2023-05-30T09:43:39.350479Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," inputs (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," dec_inputs (InputLayer)        [(None, None)]       0           []                               \n","                                                                                                  \n"," enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," encoder (Functional)           (None, None, 256)    1985792     ['inputs[0][0]',                 \n","                                                                  'enc_padding_mask[0][0]']       \n","                                                                                                  \n"," look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n","                                e)                                                                \n","                                                                                                  \n"," dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," decoder (Functional)           (None, None, 256)    2513152     ['dec_inputs[0][0]',             \n","                                                                  'encoder[0][0]',                \n","                                                                  'look_ahead_mask[0][0]',        \n","                                                                  'dec_padding_mask[0][0]']       \n","                                                                                                  \n"," outputs (Dense)                (None, None, 3639)   935223      ['decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,434,167\n","Trainable params: 5,434,167\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# initialize and compile model within strategy scope\n","with strategy.scope():\n","  model = transformer(\n","      vocab_size=VOCAB_SIZE,\n","      num_layers=NUM_LAYERS,\n","      units=UNITS,\n","      d_model=D_MODEL,\n","      num_heads=NUM_HEADS,\n","      dropout=DROPOUT)\n","\n","  model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:43:42.894878Z","iopub.status.busy":"2023-05-30T09:43:42.894487Z","iopub.status.idle":"2023-05-30T09:58:41.010599Z","shell.execute_reply":"2023-05-30T09:58:41.009731Z","shell.execute_reply.started":"2023-05-30T09:43:42.894840Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/300\n"]},{"name":"stdout","output_type":"stream","text":["175/175 [==============================] - 256s 1s/step - loss: 0.4016 - accuracy: 0.0150\n","Epoch 2/300\n","175/175 [==============================] - 257s 1s/step - loss: 0.2849 - accuracy: 0.0241\n","Epoch 3/300\n","175/175 [==============================] - 255s 1s/step - loss: 0.1934 - accuracy: 0.0357\n","Epoch 4/300\n","175/175 [==============================] - 259s 1s/step - loss: 0.1148 - accuracy: 0.0498\n","Epoch 5/300\n","175/175 [==============================] - 250s 1s/step - loss: 0.0647 - accuracy: 0.0604\n","Epoch 6/300\n","175/175 [==============================] - 242s 1s/step - loss: 0.0380 - accuracy: 0.0664\n","Epoch 7/300\n","175/175 [==============================] - 365s 2s/step - loss: 0.0248 - accuracy: 0.0697\n","Epoch 8/300\n","175/175 [==============================] - 284s 2s/step - loss: 0.0183 - accuracy: 0.0713\n","Epoch 9/300\n","175/175 [==============================] - 261s 1s/step - loss: 0.0151 - accuracy: 0.0721\n","Epoch 10/300\n","175/175 [==============================] - 271s 2s/step - loss: 0.0131 - accuracy: 0.0726\n","Epoch 11/300\n","175/175 [==============================] - 266s 2s/step - loss: 0.0118 - accuracy: 0.0729\n","Epoch 12/300\n","175/175 [==============================] - 368s 2s/step - loss: 0.0118 - accuracy: 0.0728\n","Epoch 13/300\n","175/175 [==============================] - 258s 1s/step - loss: 0.0119 - accuracy: 0.0727\n","Epoch 14/300\n","175/175 [==============================] - 273s 2s/step - loss: 0.0117 - accuracy: 0.0728\n","Epoch 15/300\n","175/175 [==============================] - 331s 2s/step - loss: 0.0102 - accuracy: 0.0731\n","Epoch 16/300\n","175/175 [==============================] - 324s 2s/step - loss: 0.0096 - accuracy: 0.0733\n","Epoch 17/300\n","175/175 [==============================] - 251s 1s/step - loss: 0.0102 - accuracy: 0.0731\n","Epoch 18/300\n","175/175 [==============================] - 246s 1s/step - loss: 0.0098 - accuracy: 0.0732\n","Epoch 19/300\n","175/175 [==============================] - 303s 2s/step - loss: 0.0102 - accuracy: 0.0731\n","Epoch 20/300\n","175/175 [==============================] - 300s 2s/step - loss: 0.0112 - accuracy: 0.0728\n","Epoch 21/300\n","175/175 [==============================] - 284s 2s/step - loss: 0.0109 - accuracy: 0.0729\n","Epoch 22/300\n","175/175 [==============================] - 261s 1s/step - loss: 0.0098 - accuracy: 0.0732\n","Epoch 23/300\n","175/175 [==============================] - 248s 1s/step - loss: 0.0085 - accuracy: 0.0735\n","Epoch 24/300\n","175/175 [==============================] - 242s 1s/step - loss: 0.0077 - accuracy: 0.0738\n","Epoch 25/300\n","175/175 [==============================] - 6685s 38s/step - loss: 0.0083 - accuracy: 0.0736\n","Epoch 26/300\n","175/175 [==============================] - 1356s 8s/step - loss: 0.0084 - accuracy: 0.0736\n","Epoch 27/300\n","175/175 [==============================] - 513s 3s/step - loss: 0.0088 - accuracy: 0.0734\n","Epoch 28/300\n","175/175 [==============================] - 245s 1s/step - loss: 0.0092 - accuracy: 0.0732\n","Epoch 29/300\n","175/175 [==============================] - 247s 1s/step - loss: 0.0091 - accuracy: 0.0732\n","Epoch 30/300\n","175/175 [==============================] - 245s 1s/step - loss: 0.0096 - accuracy: 0.0732\n","Epoch 31/300\n","175/175 [==============================] - 244s 1s/step - loss: 0.0089 - accuracy: 0.0733\n","Epoch 32/300\n","175/175 [==============================] - 246s 1s/step - loss: 0.0085 - accuracy: 0.0734\n","Epoch 33/300\n","175/175 [==============================] - 248s 1s/step - loss: 0.0076 - accuracy: 0.0737\n","Epoch 34/300\n","175/175 [==============================] - 248s 1s/step - loss: 0.0073 - accuracy: 0.0738\n","Epoch 35/300\n","175/175 [==============================] - 251s 1s/step - loss: 0.0078 - accuracy: 0.0736\n","Epoch 36/300\n","175/175 [==============================] - 250s 1s/step - loss: 0.0077 - accuracy: 0.0736\n","Epoch 37/300\n","175/175 [==============================] - 249s 1s/step - loss: 0.0081 - accuracy: 0.0735\n","Epoch 38/300\n","175/175 [==============================] - 954s 5s/step - loss: 0.0085 - accuracy: 0.0734\n","Epoch 39/300\n","175/175 [==============================] - 220s 1s/step - loss: 0.0079 - accuracy: 0.0736\n","Epoch 40/300\n","118/175 [===================>..........] - ETA: 1:16 - loss: 0.0074 - accuracy: 0.0736"]}],"source":["import datetime\n","\n","#32% - 80 epok, po 30 epokach - 29\n","\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","model.fit(dataset, epochs=300, callbacks = [tensorboard_callback])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:58:41.012528Z","iopub.status.busy":"2023-05-30T09:58:41.012149Z","iopub.status.idle":"2023-05-30T09:58:41.022367Z","shell.execute_reply":"2023-05-30T09:58:41.021729Z","shell.execute_reply.started":"2023-05-30T09:58:41.012492Z"},"trusted":true},"outputs":[],"source":["def evaluate(sentence, model):\n","#   sentence = textPreprocess(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  for i in range(MAX_LENGTH):\n","    predictions = model(inputs=[sentence, output], training=False)\n","\n","    # select the last word from the seq_len dimension\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    \n","    # return the result if the predicted_id is equal to the end token\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # concatenated the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict(sentence,model):\n","  prediction = evaluate(sentence,model)\n","\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('Input: {}'.format(sentence))\n","  print('Output: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:58:41.028008Z","iopub.status.busy":"2023-05-30T09:58:41.027454Z","iopub.status.idle":"2023-05-30T09:58:41.228510Z","shell.execute_reply":"2023-05-30T09:58:41.227600Z","shell.execute_reply.started":"2023-05-30T09:58:41.027977Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Aspire_Data\\ClassAssignment\\Semester-3\\Capstone\\Test_Project_Implement\\transformer-chatbot.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Aspire_Data/ClassAssignment/Semester-3/Capstone/Test_Project_Implement/transformer-chatbot.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39msave_weights(\u001b[39m'\u001b[39m\u001b[39msaved_weights.h5\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.save_weights('saved_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T09:58:41.231790Z","iopub.status.busy":"2023-05-30T09:58:41.231083Z","iopub.status.idle":"2023-05-30T09:58:43.899065Z","shell.execute_reply":"2023-05-30T09:58:43.898105Z","shell.execute_reply.started":"2023-05-30T09:58:41.231725Z"},"trusted":true},"outputs":[],"source":["loaded_model = transformer(\n","      vocab_size=VOCAB_SIZE,\n","      num_layers=NUM_LAYERS,\n","      units=UNITS,\n","      d_model=D_MODEL,  \n","      num_heads=NUM_HEADS,\n","      dropout=DROPOUT)\n","\n","#import h5py\n","#with h5py.File('saved_weights.h5', 'w') as f:\n","loaded_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","loaded_model.load_weights('saved_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T10:19:24.464494Z","iopub.status.busy":"2023-05-30T10:19:24.463614Z","iopub.status.idle":"2023-05-30T10:19:25.491174Z","shell.execute_reply":"2023-05-30T10:19:25.489225Z","shell.execute_reply.started":"2023-05-30T10:19:24.464459Z"},"trusted":true},"outputs":[],"source":["output = predict('love you babe', loaded_model) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
